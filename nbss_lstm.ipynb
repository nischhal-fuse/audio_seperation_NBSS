{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90667972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ness/FuseMachines/audio/speech_seperation_pytorch/audio_sep_pytorch/lib/python3.13/site-packages/torch/cuda/__init__.py:65: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0.dev20251107+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 0\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcb2cea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening 'example_mix.wav': System error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLibsndfileError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m signal[..., :length] \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;28;01melse\u001b[39;00m signal\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m mix, sr = \u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexample_mix.wav\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with real data\u001b[39;00m\n\u001b[32m     26\u001b[39m mix = torch.from_numpy(mix).float().T  \u001b[38;5;66;03m# (M, T)\u001b[39;00m\n\u001b[32m     27\u001b[39m spec = stft(mix[\u001b[32m0\u001b[39m], n_fft=\u001b[32m512\u001b[39m, hop_length=\u001b[32m256\u001b[39m)  \u001b[38;5;66;03m# (F, T)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FuseMachines/audio/speech_seperation_pytorch/audio_sep_pytorch/lib/python3.13/site-packages/soundfile.py:305\u001b[39m, in \u001b[36mread\u001b[39m\u001b[34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(file, frames=-\u001b[32m1\u001b[39m, start=\u001b[32m0\u001b[39m, stop=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[33m'\u001b[39m\u001b[33mfloat64\u001b[39m\u001b[33m'\u001b[39m, always_2d=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    220\u001b[39m          fill_value=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, samplerate=\u001b[38;5;28;01mNone\u001b[39;00m, channels=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    221\u001b[39m          \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, subtype=\u001b[38;5;28;01mNone\u001b[39;00m, endian=\u001b[38;5;28;01mNone\u001b[39;00m, closefd=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    222\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Provide audio data from a sound file as NumPy array.\u001b[39;00m\n\u001b[32m    223\u001b[39m \n\u001b[32m    224\u001b[39m \u001b[33;03m    By default, the whole file is read from the beginning, but the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    303\u001b[39m \n\u001b[32m    304\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m                   \u001b[49m\u001b[43msubtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    307\u001b[39m         frames = f._prepare_read(start, stop, frames)\n\u001b[32m    308\u001b[39m         data = f.read(frames, dtype, always_2d, fill_value, out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FuseMachines/audio/speech_seperation_pytorch/audio_sep_pytorch/lib/python3.13/site-packages/soundfile.py:690\u001b[39m, in \u001b[36mSoundFile.__init__\u001b[39m\u001b[34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28mself\u001b[39m._bitrate_mode = bitrate_mode\n\u001b[32m    688\u001b[39m \u001b[38;5;28mself\u001b[39m._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[32m    689\u001b[39m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m \u001b[38;5;28mself\u001b[39m._file = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode).issuperset(\u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.seekable():\n\u001b[32m    692\u001b[39m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[32m    693\u001b[39m     \u001b[38;5;28mself\u001b[39m.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/FuseMachines/audio/speech_seperation_pytorch/audio_sep_pytorch/lib/python3.13/site-packages/soundfile.py:1265\u001b[39m, in \u001b[36mSoundFile._open\u001b[39m\u001b[34m(self, file, mode_int, closefd)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_ptr == _ffi.NULL:\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# get the actual error code\u001b[39;00m\n\u001b[32m   1264\u001b[39m     err = _snd.sf_error(file_ptr)\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix=\u001b[33m\"\u001b[39m\u001b[33mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.name))\n\u001b[32m   1266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode_int == _snd.SFM_WRITE:\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28mself\u001b[39m._info.frames = \u001b[32m0\u001b[39m\n",
      "\u001b[31mLibsndfileError\u001b[39m: Error opening 'example_mix.wav': System error."
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "def stft(signal, n_fft=512, hop_length=256, window='hann'):\n",
    "    \"\"\"Simple STFT using torch (no librosa dependency)\"\"\"\n",
    "    win = torch.hann_window(n_fft, device=signal.device)\n",
    "    signal = F.pad(signal, (n_fft//2, n_fft//2))\n",
    "    frames = signal.unfold(1, n_fft, hop_length).transpose(0, 1)  # (T, n_fft)\n",
    "    frames = frames * win\n",
    "    spec = torch.fft.rfft(frames, dim=-1)  # (T, n_fft//2+1)\n",
    "    return spec\n",
    "\n",
    "def istft(spec, hop_length=256, length=None):\n",
    "    \"\"\"Inverse STFT\"\"\"\n",
    "    win = torch.hann_window(spec.shape[-2]*2-1, device=spec.device)\n",
    "    frames = torch.fft.irfft(spec, dim=-1)\n",
    "    frames = frames * win\n",
    "    signal = F.fold(\n",
    "        frames.transpose(0, 1).unsqueeze(1),\n",
    "        output_size=(length + hop_length, 1),\n",
    "        kernel_size=(spec.shape[-2]*2-1, 1),\n",
    "        stride=(hop_length, 1)\n",
    "    ).squeeze()\n",
    "    return signal[..., :length] if length else signal\n",
    "\n",
    "# Example\n",
    "mix, sr = sf.read(\"example_mix.wav\")  # Replace with real data\n",
    "mix = torch.from_numpy(mix).float().T  # (M, T)\n",
    "spec = stft(mix[0], n_fft=512, hop_length=256)  # (F, T)\n",
    "print(\"STFT shape (single channel):\", spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c87af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "class NBSSDataset(Dataset):\n",
    "    def __init__(self, json_file, max_len_sec=8.0, sr=16000, n_fft=512, hop=256):\n",
    "        with open(json_file) as f:\n",
    "            self.meta = json.load(f)\n",
    "        self.sr = sr\n",
    "        self.n_fft = n_fft\n",
    "        self.hop = hop\n",
    "        self.max_frames = int(max_len_sec * sr / hop)\n",
    "        self.ref_mic = 0  # reference mic\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.meta[idx]\n",
    "        mix_path = item['mixture']\n",
    "        s1_path = item['sources'][0]\n",
    "        s2_path = item['sources'][1]\n",
    "\n",
    "        # Load waveforms\n",
    "        mix, _ = sf.read(mix_path)  # (T, M)\n",
    "        s1, _ = sf.read(s1_path)    # (T,)\n",
    "        s2, _ = sf.read(s2_path)    # (T,)\n",
    "\n",
    "        T = mix.shape[0]\n",
    "        if T > self.max_frames * self.hop:\n",
    "            start = np.random.randint(0, T - self.max_frames * self.hop)\n",
    "            end = start + self.max_frames * self.hop\n",
    "            mix = mix[start:end]\n",
    "            s1 = s1[start:end]\n",
    "            s2 = s2[start:end]\n",
    "        elif T < self.max_frames * self.hop:\n",
    "            pad = self.max_frames * self.hop - T\n",
    "            mix = np.pad(mix, ((0, pad), (0, 0)))\n",
    "            s1 = np.pad(s1, (0, pad))\n",
    "            s2 = np.pad(s2, (0, pad))\n",
    "\n",
    "        mix = torch.from_numpy(mix).float()  # (T, M)\n",
    "        s1 = torch.from_numpy(s1).float()\n",
    "        s2 = torch.from_numpy(s2).float()\n",
    "\n",
    "        # STFT\n",
    "        X = torch.stft(mix, n_fft=self.n_fft, hop_length=self.hop,\n",
    "                       window=torch.hann_window(self.n_fft), return_complex=True)\n",
    "        # X: (M, F, T)\n",
    "        X = X.permute(1, 2, 0)  # (F, T, M)\n",
    "\n",
    "        Y1 = torch.stft(s1.unsqueeze(1), n_fft=self.n_fft, hop_length=self.hop,\n",
    "                        window=torch.hann_window(self.n_fft), return_complex=True)[:, self.ref_mic]\n",
    "        Y2 = torch.stft(s2.unsqueeze(1), n_fft=self.n_fft, hop_length=self.hop,\n",
    "                        window=torch.hann_window(self.n_fft), return_complex=True)[:, self.ref_mic]\n",
    "        Y = torch.stack([Y1, Y2], dim=0)  # (2, F, T)\n",
    "\n",
    "        # Normalize per frequency using reference mic magnitude\n",
    "        X_ref = X[:, :, self.ref_mic].abs().mean(dim=1, keepdim=True)  # (F, 1)\n",
    "        X_norm = X / (X_ref + 1e-8)\n",
    "\n",
    "        # Convert to real-valued\n",
    "        X_real = torch.cat([X_norm.real, X_norm.imag], dim=-1)  # (F, T, 2M)\n",
    "        Y_real = torch.cat([Y.real, Y.imag], dim=-1)            # (2, F, T)\n",
    "\n",
    "        return X_real, Y_real, X_ref.squeeze(-1)  # for denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d23dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "class NarrowBandSeparator(nn.Module):\n",
    "    def __init__(self, input_dim=16, hidden1=256, hidden2=128):  # 2M=16 for M=8\n",
    "        super().__init__()\n",
    "        self.bilstm1 = nn.LSTM(input_dim, hidden1, batch_first=True, bidirectional=True)\n",
    "        self.bilstm2 = nn.LSTM(hidden1*2, hidden2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden2*2, 4)  # 2 sources × (real + imag)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 2M)\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.fc(x)  # (B, T, 4)\n",
    "        return x.view(-1, 2, 2, x.size(1))  # (B, 2, 2, T) → real/imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d82bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def complex_from_real_imag(real_imag):\n",
    "    # real_imag: (B, 2, 2, T) → (B, 2, T) complex\n",
    "    return torch.complex(real_imag[:, :, 0], real_imag[:, :, 1])\n",
    "\n",
    "def si_sdr_loss(est, ref):\n",
    "    # est, ref: (B, T)\n",
    "    ref = ref - ref.mean(dim=1, keepdim=True)\n",
    "    est = est - est.mean(dim=1, keepdim=True)\n",
    "    alpha = (est * ref).sum(dim=1, keepdim=True) / (ref.pow(2).sum(dim=1, keepdim=True) + 1e-8)\n",
    "    s_target = alpha * ref\n",
    "    e_noise = est - s_target\n",
    "    return -10 * torch.log10(\n",
    "        s_target.pow(2).sum(dim=1) / (e_noise.pow(2).sum(dim=1) + 1e-8) + 1e-8\n",
    "    ).mean()\n",
    "\n",
    "def full_band_pit_loss(pred_complex, target_complex, X_ref):\n",
    "    \"\"\"\n",
    "    pred_complex: (B, N, F, T)\n",
    "    target_complex: (B, N, F, T)\n",
    "    X_ref: (B, F) — normalization factors\n",
    "    \"\"\"\n",
    "    B, N, F, T = pred_complex.shape\n",
    "\n",
    "    # Denormalize\n",
    "    pred = pred_complex * X_ref.view(B, 1, F, 1)\n",
    "    target = target_complex\n",
    "\n",
    "    # Reconstruct waveforms\n",
    "    pred_wav = torch.zeros(B, N, T * 256, device=pred.device)\n",
    "    target_wav = torch.zeros(B, N, T * 256, device=target.device)\n",
    "\n",
    "    for b in range(B):\n",
    "        for n in range(N):\n",
    "            pred_wav[b,n] = istft(pred[b,n], hop_length=256, length=T*256)\n",
    "            target_wav[b,n] = istft(target[b,n], hop_length=256, length=T*256)\n",
    "\n",
    "    # PIT over utterances\n",
    "    loss_perm1 = si_sdr_loss(pred_wav[:,0], target_wav[:,0]) + si_sdr_loss(pred_wav[:,1], target_wav[:,1])\n",
    "    loss_perm2 = si_sdr_loss(pred_wav[:,0], target_wav[:,1]) + si_sdr_loss(pred_wav[:,1], target_wav[:,0])\n",
    "    return torch.min(loss_perm1, loss_perm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a08a2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "class NBSSTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for X_real, Y_real, X_ref in self.train_loader:\n",
    "            X_real = X_real.to(self.device)  # (B, F, T, 2M)\n",
    "            Y_real = Y_real.to(self.device)  # (B, 2, F, T)\n",
    "            X_ref = X_ref.to(self.device)    # (B, F)\n",
    "\n",
    "            B, F, T, _ = X_real.shape\n",
    "            loss = 0\n",
    "            for f in range(F):\n",
    "                x_f = X_real[:, f]  # (B, T, 2M)\n",
    "                y_f = Y_real[:, :, f]  # (B, 2, T)\n",
    "\n",
    "                pred_f = self.model(x_f)  # (B, 2, 2, T)\n",
    "                pred_complex = complex_from_real_imag(pred_f)  # (B, 2, T)\n",
    "                target_complex = torch.complex(y_f[:, 0], y_f[:, 1])  # (B, 2, T)\n",
    "\n",
    "                # Reshape for fPIT\n",
    "                pred_complex = pred_complex.unsqueeze(2)  # (B, 2, 1, T)\n",
    "                target_complex = target_complex.unsqueeze(2)\n",
    "\n",
    "                if f == 0:\n",
    "                    pred_all = pred_complex\n",
    "                    target_all = target_complex\n",
    "                else:\n",
    "                    pred_all = torch.cat([pred_all, pred_complex], dim=2)\n",
    "                    target_all = torch.cat([target_all, target_complex], dim=2)\n",
    "\n",
    "            # Full-band PIT\n",
    "            batch_loss = full_band_pit_loss(pred_all, target_all, X_ref)\n",
    "            loss += batch_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_real, Y_real, X_ref in self.val_loader:\n",
    "                X_real = X_real.to(self.device)\n",
    "                Y_real = Y_real.to(self.device)\n",
    "                X_ref = X_ref.to(self.device)\n",
    "\n",
    "                B, F, T, _ = X_real.shape\n",
    "                pred_all = None\n",
    "                target_all = None\n",
    "                for f in range(F):\n",
    "                    x_f = X_real[:, f]\n",
    "                    y_f = Y_real[:, :, f]\n",
    "                    pred_f = self.model(x_f)\n",
    "                    pred_c = complex_from_real_imag(pred_f)\n",
    "                    targ_c = torch.complex(y_f[:, 0], y_f[:, 1])\n",
    "                    pred_c = pred_c.unsqueeze(2)\n",
    "                    targ_c = targ_c.unsqueeze(2)\n",
    "                    pred_all = pred_c if pred_all is None else torch.cat([pred_all, pred_c], dim=2)\n",
    "                    target_all = targ_c if target_all is None else torch.cat([target_all, targ_c], dim=2)\n",
    "\n",
    "                loss = full_band_pit_loss(pred_all, target_all, X_ref)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(self.val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Dummy data for demo (replace with real WSJ0-2mix spatialized)\n",
    "train_meta = [{\"mixture\": \"mix1.wav\", \"sources\": [\"s1.wav\", \"s2.wav\"]}] * 100\n",
    "val_meta = train_meta[:20]\n",
    "\n",
    "train_set = NBSSDataset(train_meta, max_len_sec=4.0)\n",
    "val_set = NBSSDataset(val_meta, max_len_sec=4.0)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=4)\n",
    "\n",
    "model = NarrowBandSeparator(input_dim=16)  # 8 mics → 2*8=16\n",
    "trainer = NBSSTrainer(model, train_loader, val_loader, device='cpu')\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss = trainer.train_epoch()\n",
    "    val_loss = trainer.validate()\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_sep_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
